activation = "relu"
hidden_units = [32, 64]
dropout = 0.2
